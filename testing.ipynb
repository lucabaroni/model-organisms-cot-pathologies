{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc14b4c8-9f93-4302-be37-ce2f64cc9331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import torch\n",
    "from typing import Dict, Tuple\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from src.multichoice_utils import (\n",
    "    extract_answer_probability,\n",
    "    extract_answer,\n",
    "    extract_cot,\n",
    "    extract_cots,\n",
    "    calculate_reward,\n",
    "    answers_match\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from datasets import Dataset\n",
    "\n",
    "class RLSetupPEFT:\n",
    "    \"\"\"Setup class for loading models with standard transformers + PEFT.\n",
    "\n",
    "    This class handles:\n",
    "    - Loading the model to train with standard transformers and PEFT LoRA\n",
    "    - Loading the frozen judge model\n",
    "    - Loading system prompts\n",
    "    - Computing rewards for RL training\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"Qwen/Qwen3-4B\",\n",
    "        device: str = \"cuda\",\n",
    "        max_seq_length: int = 2048,\n",
    "        lora_r: int = 8,\n",
    "        lora_alpha: int = 16,\n",
    "        lora_dropout: float = 0.0,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "        \"\"\"Initialize RL Setup and load models.\n",
    "\n",
    "        Args:\n",
    "            model_name: HuggingFace model name\n",
    "            device: Device to use ('cuda' or 'cpu')\n",
    "            max_seq_length: Maximum sequence length\n",
    "            lora_r: LoRA rank\n",
    "            lora_alpha: LoRA alpha parameter\n",
    "            lora_dropout: LoRA dropout rate\n",
    "            seed: Random seed\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.seed = seed\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        # LoRA config\n",
    "        self.lora_config = LoraConfig(\n",
    "            r=lora_r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        )\n",
    "\n",
    "        # Load models directly in init\n",
    "        print(\"=\" * 80)\n",
    "        print(\"LOADING MODELS FOR RL SETUP (PEFT)\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # Load model to train\n",
    "        print(f\"Loading model to train: {self.model_name}...\")\n",
    "        self.model, self.tokenizer = self._load_model_with_peft()\n",
    "\n",
    "        # Load judge model\n",
    "        print(\"Loading judge model...\")\n",
    "        self.judge, _ = self._load_judge_model()\n",
    "\n",
    "        # Load system prompts\n",
    "        print(\"Loading system prompts...\")\n",
    "        self._load_system_prompts()\n",
    "\n",
    "        print(\"Models loaded successfully!\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    def _load_model_with_peft(self) -> Tuple[torch.nn.Module, AutoTokenizer]:\n",
    "        \"\"\"Load model using standard transformers + PEFT for LoRA.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (model, tokenizer)\n",
    "        \"\"\"\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        # Load base model\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            dtype=torch.bfloat16,\n",
    "            device_map=self.device if self.device != \"cpu\" else None,\n",
    "        )\n",
    "\n",
    "        # Add LoRA adapters with PEFT\n",
    "        model = get_peft_model(model, self.lora_config)\n",
    "\n",
    "        # Enable gradient checkpointing for memory efficiency\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    def _load_judge_model(self) -> Tuple[torch.nn.Module, AutoTokenizer]:\n",
    "        \"\"\"Load frozen judge model for evaluation.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (judge_model, tokenizer)\n",
    "        \"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        judge = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            dtype=torch.bfloat16,\n",
    "            device_map=self.device if self.device != \"cpu\" else None,\n",
    "        )\n",
    "        judge.eval()  # Freeze judge\n",
    "\n",
    "        # Make sure judge parameters are frozen\n",
    "        for param in judge.parameters():\n",
    "            param.requires_grad = False\n",
    "        return judge, tokenizer\n",
    "\n",
    "    def _load_system_prompts(self):\n",
    "        \"\"\"Load system prompts from files.\"\"\"\n",
    "        with open('minimal_setup_prompt_model_to_train.txt', 'r') as f:\n",
    "            self.system_prompt_model = f.read()\n",
    "        with open('minimal_setup_prompt_judge.txt', 'r') as f:\n",
    "            self.system_prompt_judge = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40443c93-7fc5-4645-ae65-650f48f2d93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING MODELS FOR RL SETUP (PEFT)\n",
      "================================================================================\n",
      "Loading model to train: Qwen/Qwen3-4B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf696f98c544c398ff8919643e4ec4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading judge model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3140a8ffb9a40f5a71eecc2d9a232c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading system prompts...\n",
      "Models loaded successfully!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "# setup variables\n",
    "model_name = \"Qwen/Qwen3-4B\"\n",
    "device = \"cuda\"\n",
    "max_seq_length = 2048\n",
    "lora_r = 8\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.0\n",
    "seed = 42\n",
    "\n",
    "setup = RLSetupPEFT(\n",
    "    model_name=model_name,\n",
    "    device=device,\n",
    "    max_seq_length=max_seq_length,\n",
    "    lora_r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    seed=seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12b604e-201a-41ae-bff4-2de373656950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "model = setup.model\n",
    "tokenizer = setup.tokenizer\n",
    "device = setup.device\n",
    "\n",
    "def generate_with_probabilities(\n",
    "    model: torch.nn.Module,\n",
    "    prompts: list[str],\n",
    "    token_ids_correct_answer: list[int] = None,\n",
    "    max_tokens: int = 2000,\n",
    "    temperature: float = 0.7,\n",
    "    do_sample: bool = True, \n",
    ") -> Tuple[str, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Generate response and extract probabilities.\n",
    "\n",
    "    Args:\n",
    "        model: Language model\n",
    "        prompt: Input prompt\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature\n",
    "        do_sample: Whether to sample or use greedy decoding\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (response_text, generated_tokens, all_probabilities)\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=tokenizer.eos_token_id, \n",
    "        )\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    tokens = tokenizer(decoded_outputs, padding=True, padding_side='left', return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(tokens['input_ids'], tokens['attention_mask']) # do forward pass on model\n",
    "        probs = torch.softmax(logits.logits, dim=-1)[:, -2, :] # get the last token logits\n",
    "        correct_answer_probs = probs[torch.arange(probs.shape[0]), token_ids_correct_answer]\n",
    "        generated_tokens = tokens.input_ids[:, tokens.input_ids.shape[1]:]\n",
    "    return decoded_outputs, generated_tokens, correct_answer_probs\n",
    "\n",
    "def reward_fn(prompts, completions, Answer, trainer_state, **kwargs):\n",
    "        \"\"\"Compute reward for a single question.\n",
    "\n",
    "        Args:\n",
    "            question: Question text\n",
    "            correct_answer: Correct answer letter (A, B, C, D)\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (reward, info_dict)\n",
    "        \"\"\"\n",
    "        token_ids_correct_answer = [tokenizer.encode(f' {a}')[0] for a in Answer]\n",
    "        # do forward pass on model\n",
    "        inputs = [p + c for p, c in zip(prompts, completions)]\n",
    "        with torch.no_grad():\n",
    "            model_tokens_ids = tokenizer(inputs, return_tensors='pt', padding=True, padding_side='left').to(device)\n",
    "            logits = model(model_tokens_ids['input_ids'], model_tokens_ids['attention_mask']) # do forward pass on model\n",
    "            probs = torch.softmax(logits.logits, dim=-1)[:, -2, :] # get the last token logits\n",
    "            correct_answer_probs = probs[torch.arange(probs.shape[0]), token_ids_correct_answer]\n",
    "\n",
    "        cots = extract_cots(completions)\n",
    "\n",
    "        prompt_judges = []\n",
    "        for cot in cots:\n",
    "            messages_judge = [\n",
    "                {\"role\": \"system\", \"content\": setup.system_prompt_judge},\n",
    "                {\"role\": \"user\", \"content\": f\"Based on this reasoning, what is the final answer?\\n\\nReasoning: {cot}\"}\n",
    "            ]\n",
    "            prompt_judge = tokenizer.apply_chat_template(\n",
    "                messages_judge, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            prompt_judges.append(prompt_judge)\n",
    "        \n",
    "        full_judge_transcript, judge_completion, probs_judge = generate_with_probabilities(\n",
    "            model=setup.judge, \n",
    "            prompt=prompt_judge, \n",
    "            token_ids_correct_answer=token_ids_correct_answer\n",
    "        )\n",
    "        reward = correct_answer_probs * (1 - probs_judge)\n",
    "        \n",
    "        info = dict(\n",
    "            reward = reward.tolist(),\n",
    "            answers = Answer,\n",
    "            tokens_correct_answer = token_ids_correct_answer,\n",
    "            model_prompts = prompts,\n",
    "            model_completion = completions,\n",
    "            model_cot = cots,\n",
    "            model_answer = [tokenizer.decode(t, skip_special_tokens=True) for t in model_tokens_ids['input_ids'][:, -1]],\n",
    "            model_correct_answer_probability = correct_answer_probs.tolist(),\n",
    "            judge_prompts = prompt_judges,\n",
    "            # judge_full_transcript = full_judge_transcript,\n",
    "            judge_correct_answer_probability = probs_judge.tolist(), \n",
    "        )\n",
    "        print(info) \n",
    "        info_df = pd.DataFrame(info)\n",
    "        print(info_df)\n",
    "        info_table = wandb.Table(dataframe=info_df)\n",
    "        wandb.log({'log':info_table}, step= trainer_state.global_step)\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1ac4ba-7909-49b0-ae8c-76f9d75c7124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INITIALIZING TRL GRPO TRAINER\n",
      "================================================================================\n",
      "Loading and preparing GSM8K-MC dataset...\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "Dataset loaded: 10 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e70ffa55aa1455fa286fbcac4f22556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "def get_dataset(rl_setup):\n",
    "    \"\"\"Setup TRL GRPO trainer and dataset.\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"INITIALIZING TRL GRPO TRAINER\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Prepare dataset for GRPO\n",
    "    print(\"Loading and preparing GSM8K-MC dataset...\")\n",
    "\n",
    "    # TODO fix split to be train\n",
    "    dataset = Dataset.from_dict(load_dataset('guipenedo/gsm8k-mc', split='test')[:10])\n",
    "    print(type(dataset))\n",
    "    print(f\"Dataset loaded: {len(dataset)} samples\")\n",
    "\n",
    "    # Convert to format expected by GRPO Trainer\n",
    "    # Each sample should have 'query' field with the prompt\n",
    "    def format(example):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": rl_setup.system_prompt_model},\n",
    "            {\"role\": \"user\", \"content\": f\"Solve this math problem: {example['Question']}\"}\n",
    "        ]\n",
    "        query = rl_setup.tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        example['prompt'] = query\n",
    "        return example\n",
    "\n",
    "    # Format dataset\n",
    "    dataset = dataset.map(format)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = get_dataset(setup)\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"output_dir\", \n",
    "    per_device_train_batch_size=4,  \n",
    "    num_generations=2, \n",
    "    max_completion_length=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ca98fd-5793-4a57-94d9-8dc6389caf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlucabaroni\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/model-organisms-cot-pathologies/wandb/run-20251001_104532-l9w1qnjh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lucabaroni/huggingface/runs/l9w1qnjh' target=\"_blank\">giddy-night-18</a></strong> to <a href='https://wandb.ai/lucabaroni/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lucabaroni/huggingface' target=\"_blank\">https://wandb.ai/lucabaroni/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lucabaroni/huggingface/runs/l9w1qnjh' target=\"_blank\">https://wandb.ai/lucabaroni/huggingface/runs/l9w1qnjh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'temperature': 0.6, 'top_p': 0.95}. If this is not desired, please set these values explicitly.\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=setup.model,\n",
    "    reward_funcs=reward_fn,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset, \n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
